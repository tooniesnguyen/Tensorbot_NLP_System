from nltk.tokenize import word_tokenize

class Tokenize:
    def __init__(self):
        self.tokenize_en = word_tokenize()

    def tokenize_en(self, sentence):
        '''
        Tokenize english sentence
        Ouput: list
        '''
        return self.tokenize_en(sentence)
    
